dataset:
  name: textvqa
  root: ./data_pipeline/data/textvqa
  batch_size: 4
  num_workers: 2
corruption:
  blur_prob: 0.3
  occlusion_prob: 0.3
  crop_prob: 0.2
  ocr_noise_prob: 0.4
retrieval:
  top_k: 5
  noise_threshold: 0.6
model:
  backbone: simple_decoder  # options: simple_decoder (mock), qwen3-vl (real backbone)
  hidden_size: 512          # adapter hidden size when running the lightweight mock backbone
  question_length: 64
  vision_tokens: 32
  prefix_length: 64
  memory_tokens: 32
  imputation_tokens: 16
  vocab_size: 16384
  gate_default: 0.65
  qwen3_vl:
    text_hidden_size: 4096      # real Qwen3-VL language tower width
    vision_hidden_size: 1664    # CLIP-ViT-L/14 projector width
    num_layers: 48
    num_attention_heads: 32
    vocab_size: 152064
    max_position_embeddings: 32768
training:
  epochs: 1
  lr_backbone: 1.0e-5
  lr_adapter: 5.0e-5
  log_interval: 20
evaluation:
  benchmarks:
    - textvqa
    - chartqa
    - docvqa
